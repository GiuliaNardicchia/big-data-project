{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Obiettivo del progetto\n",
    "Dopo aver analizzato e compreso i dati, si vuole studiare meglio la correlazione riscontrata tra distanza e prezzo e si è individuato l'obiettivo del progetto. L'obiettivo è quello di verificare se c’è una stagionalità, nella quale i prezzi per alcuni mesi sono molto più elevati rispetto ad altri o se ci sono grandi variazioni di prezzo tra i diversi mesi rispetto alle diverse distanze."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "529d658979e1113d"
  },
  {
   "cell_type": "markdown",
   "id": "181bcbee1aae90f6",
   "metadata": {},
   "source": [
    "## Descrizione del job proposto\n",
    "Avendo a disposizione un solo file *.csv* si è pensato si usare un pattern di tipo *self-join*:\n",
    "\n",
    "-\t**Prima aggregazione**: aggregare per ogni combinazione di aeroporto di partenza e destinazione (*startingAeroport* e *destinationAeroport*) per ottenere la distanza media di viaggio (*totalTravelDistance*). A partire dalla distanza media generare una nuova colonna che indichi la fascia di distanza del volo (breve distanza, media distanza, lunga distanza);\n",
    "\n",
    "-\t**Join**: unire il dataset originale con il risultato ottenuto;\n",
    "\n",
    "-\t**Seconda aggregazione**: aggregare per fascia di distanza e mese (*flightDate*, da cui si ricava il mese) per ottenere per ciascuna combinazione il prezzo medio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae506c8e35674206",
   "metadata": {},
   "source": [
    "### Caricamento libreria Spark\n",
    "\n",
    "Per prima cosa, si deve importare la libreria spark per avviare una `spark-shell`; in seguito verrà mostrato il link tramite il quale è possibile accedere all'interfaccia utente di Spark."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "import org.apache.spark",
   "id": "672abbdbb9ccdca0"
  },
  {
   "cell_type": "code",
   "id": "c5f69bc549d0a39e",
   "metadata": {},
   "source": [
    "// DO NOT EXECUTE - this is needed just to avoid showing errors in the following cells\n",
    "val sc = spark.SparkContext.getOrCreate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "93c45630c708235f",
   "metadata": {},
   "source": [
    "### Parser del file .csv\n",
    "\n",
    "Nella cella sottostante è implementata una `case class Flight` con come parametri tutte le colonne(*) presenti nel file .csv descritto nel notebook [data-exploration.ipynb](./data-exploration.ipynb) e un `FlightParser` che consentente l'estrazione delle informazioni necessarie per popolare l'oggetto RDD di Spark.\n",
    "\n",
    "(*) per risolvere il `job` proposto verranno utilizzate solo alcune delle colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a8f0689-42d1-479c-9878-d8323f6f3fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.text.SimpleDateFormat\r\n",
       "import java.util.Calendar\r\n",
       "defined class Flight\r\n",
       "defined object FlightParser\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.text.SimpleDateFormat\n",
    "import java.util.Calendar\n",
    "\n",
    "/**\n",
    " * Flight case class.\n",
    " */\n",
    "case class Flight(\n",
    "    legId: String,\n",
    "    searchMonth: Int,\n",
    "    flightMonth: Int,\n",
    "    startingAirport: String,\n",
    "    destinationAirport: String,\n",
    "    fareBasisCode: String,\n",
    "    travelDuration: String,\n",
    "    elapsedDays: Int,\n",
    "    isBasicEconomy: Boolean,\n",
    "    isRefundable: Boolean,\n",
    "    isNonStop: Boolean,\n",
    "    baseFare: Double,\n",
    "    totalFare: Double,\n",
    "    seatsRemaining: Int,\n",
    "    totalTravelDistance: Double,\n",
    "    segmentsDepartureTimeEpochSeconds: String,\n",
    "    segmentsDepartureTimeRaw: String,\n",
    "    segmentsArrivalTimeEpochSeconds: String,\n",
    "    segmentsArrivalTimeRaw: String,\n",
    "    segmentsArrivalAirportCode: String,\n",
    "    segmentsDepartureAirportCode: String,\n",
    "    segmentsAirlineName: String,\n",
    "    segmentsAirlineCode: String,\n",
    "    segmentsEquipmentDescription: String,\n",
    "    segmentsDurationInSeconds: String,\n",
    "    segmentsDistance: String,\n",
    "    segmentsCabinCode: String\n",
    ") extends Serializable\n",
    "\n",
    "/**\n",
    " * Flight parser.\n",
    " */\n",
    "object FlightParser extends Serializable {\n",
    "\n",
    "  val comma = \",\"\n",
    "\n",
    "  /**\n",
    "   * Convert from date (String) to month (Int).\n",
    "   * @param dateString the date\n",
    "   * @return the month\n",
    "   */\n",
    "  def monthFromDate(dateString: String): Int = {\n",
    "    val sdf = new SimpleDateFormat(\"yyyy-MM-dd\")\n",
    "    val date = sdf.parse(dateString.trim)\n",
    "    val cal = Calendar.getInstance()\n",
    "    cal.setTime(date)\n",
    "    cal.get(Calendar.MONTH) + 1\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Function to parse flights records.\n",
    "   * @param line that has to be parsed\n",
    "   * @return Flight object, None in case of input errors\n",
    "   */\n",
    "  def parseFlightLine(line: String): Option[Flight] = {\n",
    "    try {\n",
    "      val columns = line.split(comma)\n",
    "      Some(\n",
    "        Flight(\n",
    "          legId = columns(0).trim,\n",
    "          searchMonth = monthFromDate(columns(1)),\n",
    "          flightMonth = monthFromDate(columns(2)),\n",
    "          startingAirport = columns(3).trim,\n",
    "          destinationAirport = columns(4).trim,\n",
    "          fareBasisCode = columns(5).trim,\n",
    "          travelDuration = columns(6).trim,\n",
    "          elapsedDays = columns(7).trim.toInt,\n",
    "          isBasicEconomy = columns(8).trim.toBoolean,\n",
    "          isRefundable = columns(9).trim.toBoolean,\n",
    "          isNonStop = columns(10).trim.toBoolean,\n",
    "          baseFare = columns(11).trim.toDouble,\n",
    "          totalFare = columns(12).trim.toDouble,\n",
    "          seatsRemaining = columns(13).trim.toInt,\n",
    "          totalTravelDistance = columns(14).trim.toDouble,\n",
    "          segmentsDepartureTimeEpochSeconds = columns(15).trim,\n",
    "          segmentsDepartureTimeRaw = columns(16).trim,\n",
    "          segmentsArrivalTimeEpochSeconds = columns(17).trim,\n",
    "          segmentsArrivalTimeRaw = columns(18).trim,\n",
    "          segmentsArrivalAirportCode = columns(19).trim,\n",
    "          segmentsDepartureAirportCode = columns(20).trim,\n",
    "          segmentsAirlineName = columns(21).trim,\n",
    "          segmentsAirlineCode = columns(22).trim,\n",
    "          segmentsEquipmentDescription = columns(23).trim,\n",
    "          segmentsDurationInSeconds = columns(24).trim,\n",
    "          segmentsDistance = columns(25).trim,\n",
    "          segmentsCabinCode = columns(26).trim\n",
    "        )\n",
    "      )\n",
    "    } catch {\n",
    "      case e: Exception =>\n",
    "        // println(s\"Errore durante il parsing della riga '$line': ${e.getMessage}\")\n",
    "        None\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d195400e1cea1b18",
   "metadata": {},
   "source": [
    "### Caricamento dei dati\n",
    "\n",
    "Con la seguente cella si effettua il caricamento del file *itineraries-sample\\<N\\>.csv*, dove con N si intende la percentuale di dati campionati dal file originale di 31,09 GB.\n",
    "\n",
    "I file disponibili sono scaricabili dalla cartella su [OneDrive](https://liveunibo-my.sharepoint.com/:f:/g/personal/giulia_nardicchia_studio_unibo_it/Ei2686kRO3JFrY-4LnImGpwBtge9FRErDnIgvT2h2QB-Pg?e=VrufWl) e hanno percentuale: `02`, `16` e `33`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8681b935c9353693",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:05:33.101282Z",
     "start_time": "2025-01-29T15:05:32.939767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasetsPath: String = ../../../datasets/big/\r\n",
       "fileName: String = itineraries-sample16.csv\r\n",
       "rawData: org.apache.spark.rdd.RDD[String] = ../../../datasets/big/itineraries-sample16.csv MapPartitionsRDD[79] at textFile at <console>:43\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datasetsPath = \"../../../datasets/big/\"\n",
    "val fileName = \"itineraries-sample16.csv\"\n",
    "\n",
    "val rawData = sc.textFile(datasetsPath + fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad3c19d0de51fa",
   "metadata": {},
   "source": [
    "Trasformazione di un RDD composto da dati grezzi (*rawData*) in un RDD di oggetti `Flight`. La funzione `FlightParser.parseFlightLine` analizza ogni riga. `flatMap` appiattisce i risultati, scartando automaticamente le righe non valide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba8c35cd3020a7a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T12:58:15.695017Z",
     "start_time": "2025-01-29T12:58:14.453101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddFlights: org.apache.spark.rdd.RDD[Flight] = MapPartitionsRDD[2] at flatMap at <console>:28\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rddFlights = rawData.flatMap(FlightParser.parseFlightLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd44f9230e1434",
   "metadata": {},
   "source": [
    "Per verificare che non ci siano stati problemi di *parsing*, con la cella seguente si vuole eseguire un'azione. La funzione `count()` calcola il numero di righe valide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "411a20cf90856722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T12:58:25.512140Z",
     "start_time": "2025-01-29T12:58:19.712226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Long = 1520662\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddFlights.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1500dc766c51eab",
   "metadata": {},
   "source": [
    "### Prima aggregazione\n",
    "\n",
    "Innanzitutto si utilizza `map` per ridurre i dati da manipolare, quindi vengono scartate tutte le colonne che non servono a svolgere il job proposto e per trasformare i dati di tipo (chiave, valore). Si vuole aggregare per ogni combinazione di aeroporto di partenza e destinazione (*startingAeroport* e *destinationAeroport*) per ottenere la distanza media di viaggio (*totalTravelDistance*). Per fare questo si utilizza la funzione `aggregateByKey()` passando come argomenti: (0.0, 0), i due valori iniziali, una funzione *combining* (*map*) per indicare come i singoli valori sono combinati con l'accumulatore e una funzione di *merging* (*reduce*) per indicare come gli accumulatori per le differenti partizioni devono essere uniti. Come funzioni di *combining* sono state passate due funzioni **associative** e **commutative** quali la somma e il conteggio, una volta ottenuti questi risultati si effettua il rapporto per calcolare la media delle distanze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb3928b8618a571d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T12:58:30.524285Z",
     "start_time": "2025-01-29T12:58:29.154674Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgDistances: org.apache.spark.rdd.RDD[((String, String), Double)] = MapPartitionsRDD[5] at mapValues at <console>:33\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val avgDistances = rddFlights\n",
    "  .map(flight => ((flight.startingAirport, flight.destinationAirport), flight.totalTravelDistance))\n",
    "  .aggregateByKey((0.0, 0))(\n",
    "    (acc, travelDistance) => (acc._1 + travelDistance, acc._2 + 1),\n",
    "    (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    "  )\n",
    "  .mapValues { case (sumDistance, count) => sumDistance / count }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22f52f39-34a4-4c7c-832f-f40b96ce0570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T12:58:38.916672Z",
     "start_time": "2025-01-29T12:58:33.266297Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[((String, String), Double)] = Array(((BOS,LGA),406.6977958842578), ((IAD,ORD),841.525204359673), ((EWR,PHL),1039.5994575045208), ((DTW,LGA),694.1526669795088), ((OAK,DFW),2123.889001864091), ((ATL,DEN),1513.575124745888), ((IAD,CLT),587.9657102869139), ((DEN,LGA),1804.57909562639), ((DTW,EWR),736.2682451253482), ((LGA,DFW),1455.155069582505), ((OAK,JFK),3126.575707702436), ((DEN,DTW),1578.6580700623254), ((JFK,IAD),703.4175354183374), ((ORD,MIA),1521.1334047682828), ((IAD,DFW),1363.3681891954557), ((DEN,PHL),1852.40172900494), ((OAK,DEN),1419.471807628524), ((BOS,JFK),261.94046744083494), ((SFO,JFK),2652.746982695943), ((DTW,MIA),1462.8436163714111), ((PHL,OAK),2949.3589503280223), ((CLT,LGA),665.3443708609271), ((DTW,JFK),842.9147381242387), ((ATL,IAD),647.0074156470153), (...\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgDistances.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f64718cfaaeafc",
   "metadata": {},
   "source": [
    "A partire dalla distanza media, si intende generare una nuova colonna che specifichi la fascia di distanza del volo, distinguendo tra breve distanza, media distanza e lunga distanza.\n",
    "\n",
    "Poiché usare valori numerici *hard coded* rappresenta una *bad practice*, si è deciso di utilizzare il minimo, il massimo e il numero di classi per calcolare dinamicamente l'intervallo delle fasce di distanza. Tuttavia, si è consapevoli che, nel caso in cui il codice venisse riutilizzato su un diverso dataset, gli intervalli calcolati potrebbero perdere significato. Le distanze, infatti, potrebbero variare significativamente, facendo sì che uno stesso percorso, classificato come breve in un dataset, risulti invece di media o lunga distanza in un altro.\n",
    "\n",
    "Per ottenere il minimo e il massimo su `avgDistances` appena calcolato, si utilizza la funzione `aggregate()` specificando come valore iniziale per il minimo `Double.MaxValue` mentre per il massimo `Double.MinValue`. Per confrontare, invece, i valori tra di loro si utilizza il package `scala.math` con la funzione `math.min()` e `math.max()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b489572-22b4-4038-a7fa-0081398ed80d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T13:53:42.822952Z",
     "start_time": "2025-01-29T13:53:42.267638Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minDistance: Double = 185.0\r\n",
       "maxDistance: Double = 3366.947416137806\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val (minDistance, maxDistance) = avgDistances\n",
    "    .aggregate((Double.MaxValue, Double.MinValue))(\n",
    "        (acc, value) => (math.min(acc._1, value._2), math.max(acc._2, value._2)),\n",
    "        (acc1, acc2) => (math.min(acc1._1, acc2._1), math.max(acc1._2, acc2._2))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d113be73d8f55809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T13:53:45.432268Z",
     "start_time": "2025-01-29T13:53:45.159172Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numClasses: Int = 3\r\n",
       "range: Double = 1060.649138712602\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numClasses = 3\n",
    "val range = (maxDistance - minDistance) / numClasses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c5f70d0a66232",
   "metadata": {},
   "source": [
    "Per calcolare l'intervallo in maniera equidistante sono stati adottati i seguenti limiti:\n",
    "- **Breve**: se la distanza media è inferiore a *minimo + intervallo*;\n",
    "- **Media**: se la distanza media è compresa tra *[minimo + intervallo; minimo + 2 * intervallo)*;\n",
    "- **Lunga**: se la distanza media è superiore a *minimo + (numero classi - 1) * intervallo*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d48c6d16984e76a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T13:53:59.064315Z",
     "start_time": "2025-01-29T13:53:58.568465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifiedDistances: org.apache.spark.rdd.RDD[((String, String), String)] = MapPartitionsRDD[6] at mapValues at <console>:30\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val classifiedDistances = avgDistances.mapValues {\n",
    "  case (avgDistance) => \n",
    "     val classification = if (avgDistance < minDistance + range) \"short\"\n",
    "     else if (avgDistance <= minDistance + (numClasses - 1) * range ) \"medium\"\n",
    "     else \"long\"\n",
    "    classification\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f60e39b-ecff-4bcf-ab28-074ba9e0d677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T13:54:01.572975Z",
     "start_time": "2025-01-29T13:54:00.494604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[((String, String), String)] = Array(((BOS,LGA),short), ((IAD,ORD),short), ((EWR,PHL),short), ((DTW,LGA),short), ((OAK,DFW),medium), ((ATL,DEN),medium), ((IAD,CLT),short), ((DEN,LGA),medium), ((DTW,EWR),short), ((LGA,DFW),medium), ((OAK,JFK),long), ((DEN,DTW),medium), ((JFK,IAD),short), ((ORD,MIA),medium), ((IAD,DFW),medium), ((DEN,PHL),medium), ((OAK,DEN),medium), ((BOS,JFK),short), ((SFO,JFK),long), ((DTW,MIA),medium), ((PHL,OAK),long), ((CLT,LGA),short), ((DTW,JFK),short), ((ATL,IAD),short), ((ATL,MIA),short), ((DTW,IAD),short), ((OAK,LGA),long), ((SFO,EWR),long), ((IAD,SFO),long), ((CLT,SFO),long), ((BOS,ATL),short), ((LAX,DEN),short), ((DEN,JFK),medium), ((BOS,LAX),long), ((SFO,IAD),long), ((DTW,DEN),medium), ((ORD,LGA),short), ((ATL,OAK),long), ((MIA,CLT),short), ((EWR,...\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifiedDistances.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24023d49bd4355e",
   "metadata": {},
   "source": [
    "### Join + Seconda aggregazione\n",
    "\n",
    "Si unisce il dataset originale con il risultato ottenuto, aggregando poi per fascia di distanza e mese (*flightDate*, da cui si ricava il mese) si ottiene per ciascuna combinazione il prezzo medio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc88d7785805c488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T13:54:06.420505Z",
     "start_time": "2025-01-29T13:54:05.164163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultJob: org.apache.spark.rdd.RDD[(Int, String, Double)] = MapPartitionsRDD[13] at map at <console>:35\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val resultJob = rddFlights\n",
    "  .map(flight => ((flight.startingAirport, flight.destinationAirport), (flight.flightMonth, flight.totalFare)))\n",
    "  .join(classifiedDistances)\n",
    "  .map {\n",
    "    case (_, ((flightMonth, totalFare), classification)) => ((flightMonth, classification), (totalFare, 1))\n",
    "  }\n",
    "  .reduceByKey((acc, totalFare) => (acc._1 + totalFare._1, acc._2 + totalFare._2))\n",
    "  .map {\n",
    "    case ((flightMonth, classification), (sumTotalFare, count)) => (flightMonth, classification, sumTotalFare / count)\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cfcbba0-59f0-42fa-a35f-93b1b178ebc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T13:54:16.074844Z",
     "start_time": "2025-01-29T13:54:09.896977Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[(Int, String, Double)] = Array((9,long,405.4846256093827), (11,short,222.00556413449337), (11,long,382.2554902106178), (5,long,533.1691284153799), (7,short,290.6924974546775), (9,short,256.1263844227748), (4,long,480.8690961538476), (9,medium,293.22341323056406), (11,medium,254.47777319902355), (8,medium,322.54506223357464), (7,long,554.1280932802313), (6,long,598.8304861754743), (8,short,265.2235958197947), (5,medium,354.6517011517386), (10,short,259.31215532219113), (7,medium,382.5014410856043), (10,long,411.65239396939035), (5,short,278.2944923340169), (8,long,462.04696232908327), (6,short,296.05174334065094), (4,short,306.4335681610265), (6,medium,395.04683677556426), (10,medium,294.67349021285935), (4,medium,331.0029844885145))\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultJob.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bfd7ca0e66fbb2",
   "metadata": {},
   "source": [
    "## Job Not Optimized\n",
    "L'ordine con cui sono state eseguite alcune operazioni, è migliorabile e quindi a partire dal codice scritto nelle precedenti celle, si è proceduto a *\"rifattorizzare\"* l'implementazione. Di seguito il *job* proposto non ottimizzato, in cui il nome delle variabili usate è il medesimo con l'aggiunta del suffisso NO (*Not Optimized*).\n",
    "\n",
    "Differenze:\n",
    "\n",
    "   - Innanzitutto, è stato migliorato il *mapping* iniziale, così da evitare di eseguire un ulteriore *mapping* in seguito per effettuare il *join*. In pratica vengono estratte le colonne a cui si è interessati subito (*totalTravelDistance, flightMonth, totalFare*) e viene usato quello come dataset di partenza.\n",
    "\n",
    "   - Il *join*, adesso viene eseguito sul risultato ottenuto con la classificazione, al contrario della modalità precedente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "004a6651-59d3-4191-8d5b-a9e60c15ed6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T13:54:48.996328Z",
     "start_time": "2025-01-29T13:54:43.716720Z"
    },
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numClassesNO: Int = 3\r\n",
       "rddFlightsNO: org.apache.spark.rdd.RDD[((String, String), (Double, Int, Double))] = MapPartitionsRDD[15] at map at <console>:32\r\n",
       "avgDistancesNO: org.apache.spark.rdd.RDD[((String, String), Double)] = MapPartitionsRDD[17] at mapValues at <console>:41\r\n",
       "minDistanceNO: Double = 185.0\r\n",
       "maxDistanceNO: Double = 3366.947416137806\r\n",
       "rangeNO: Double = 1060.649138712602\r\n",
       "resultJobNotOptimized: org.apache.spark.rdd.RDD[(Int, String, Double)] = MapPartitionsRDD[24] at map at <console>:60\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numClassesNO = 3\n",
    "\n",
    "val rddFlightsNO = rawData.flatMap(FlightParser.parseFlightLine)\n",
    "    // (k,v) => (startingAirport, destinationAirport), (totalTravelDistance, flightDate, totalFare))\n",
    "    .map(flight => ((flight.startingAirport, flight.destinationAirport),\n",
    "                    (flight.totalTravelDistance, flight.flightMonth, flight.totalFare)))\n",
    "\n",
    "val avgDistancesNO = rddFlightsNO\n",
    "    .aggregateByKey((0.0, 0))(\n",
    "        (acc, travelDistance) => (acc._1 + travelDistance._1, acc._2 + 1),\n",
    "        (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    "    )\n",
    "    // (k,v) => ((startingAirport, destinationAirport), avgDistance)\n",
    "    .mapValues { case (sumDistance, count) => sumDistance / count }\n",
    "\n",
    "val (minDistanceNO, maxDistanceNO) = avgDistancesNO\n",
    "    .aggregate((Double.MaxValue, Double.MinValue))(\n",
    "        (acc, avgDistance) => (math.min(acc._1, avgDistance._2), math.max(acc._2, avgDistance._2)),\n",
    "        (acc1, acc2) => (math.min(acc1._1, acc2._1), math.max(acc1._2, acc2._2))\n",
    "    )\n",
    "\n",
    "val rangeNO = (maxDistanceNO - minDistanceNO) / numClassesNO\n",
    "\n",
    "val resultJobNotOptimized = avgDistancesNO\n",
    "    .mapValues {\n",
    "      case d if d < minDistanceNO + rangeNO => \"short\"\n",
    "      case d if d < minDistanceNO + (numClassesNO - 1) * rangeNO => \"medium\"\n",
    "      case _ => \"long\"\n",
    "    } // (k,v) => ((startingAirport, destinationAirport), classification)\n",
    "    .join(rddFlightsNO)\n",
    "    .map { case (_, (classification, (_, month, totalFare))) => ((month, classification), (totalFare, 1)) }\n",
    "    .reduceByKey((acc, totalFare) => (acc._1 + totalFare._1, acc._2 + totalFare._2))\n",
    "    .map { case ((month, classification), (sumTotalFare : Double, count: Int)) => (month, classification, sumTotalFare / count) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0efbfcf7-5bb8-48f6-9a99-573b9edbb153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T13:54:59.597631Z",
     "start_time": "2025-01-29T13:54:52.029135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[(Int, String, Double)] = Array((9,long,405.4846256093826), (11,short,222.00556413449337), (11,long,382.2554902106179), (5,long,533.1691284153799), (7,short,290.69249745467766), (9,short,256.1263844227749), (4,long,480.8690961538476), (9,medium,293.22341323056395), (11,medium,254.4777731990232), (8,medium,322.545062233574), (7,long,554.1280932802313), (6,long,598.8304861754746), (8,short,265.223595819795), (5,medium,354.65170115173936), (10,short,259.3121553221906), (7,medium,382.50144108560403), (10,long,411.65239396939035), (5,short,278.294492334017), (8,long,462.04696232908344), (6,short,296.05174334065117), (4,short,306.4335681610265), (6,medium,395.0468367755642), (10,medium,294.67349021286043), (4,medium,331.0029844885145))\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultJobNotOptimized.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac4cd4519a9570b",
   "metadata": {},
   "source": [
    "- Cache/Persist\n",
    "- Repartition/PartitionBy\n",
    "- Broadcast variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f46674-634a-458a-ab23-e48cdda23546",
   "metadata": {},
   "source": [
    "# Considerazioni sulle ottimizzazioni\n",
    "\n",
    "In questa sezione si vogliono fare alcune considerazioni riguardo le ottimizzazioni."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d7eae4d449c4d9",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "Nel job proposto, le partizioni applicate di *default* sono 19 da 32MB ciascuno.\n",
    "\n",
    "In locale, essendoci 4 core a disposizione, vengono assegnate 5 partizioni per core (circa).\n",
    "\n",
    "In remoto, essendoci 6*2 = 12 core a disposizione, vengono assegnate 2 partizioni per core (circa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3bc2fbd8d752bfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T13:56:05.786031Z",
     "start_time": "2025-01-29T13:56:01.463558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\r\n",
      "19\r\n",
      "(0,84320)\r\n",
      "(1,84270)\r\n",
      "(2,82909)\r\n",
      "(3,80423)\r\n",
      "(4,83522)\r\n",
      "(5,83474)\r\n",
      "(6,83326)\r\n",
      "(7,74715)\r\n",
      "(8,83102)\r\n",
      "(9,83171)\r\n",
      "(10,82888)\r\n",
      "(11,82202)\r\n",
      "(12,81915)\r\n",
      "(13,82019)\r\n",
      "(14,81801)\r\n",
      "(15,81669)\r\n",
      "(16,81562)\r\n",
      "(17,80749)\r\n",
      "(18,42625)\r\n"
     ]
    }
   ],
   "source": [
    "println(rddFlightsNO.partitioner)\n",
    "println(rddFlightsNO.partitions.length)\n",
    "rddFlightsNO.mapPartitionsWithIndex((index, iter) => Iterator((index, iter.size))).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2914a2-411f-459c-b825-513953211cb3",
   "metadata": {},
   "source": [
    "## Job Optimized\n",
    "\n",
    "Una volta capite quali tecniche di ottimizzazione sono da adottare per migliorare le performance, sia in termini di CPU sia in termini di memoria, è stato riscritto il *main job* e si è ottenuto il seguente codice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9213ee75-718d-49ef-99d8-e84dc43cbd67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:12:20.257940Z",
     "start_time": "2025-01-29T15:11:09.922916Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.storage.StorageLevel._\r\n",
       "import org.apache.spark.HashPartitioner\r\n",
       "numClassesO: Int = 3\r\n",
       "numPartitions: Int = 12\r\n",
       "p: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@c\r\n",
       "rddFlightsO: org.apache.spark.rdd.RDD[((String, String), (Double, Int, Double))] = ShuffledRDD[95] at partitionBy at <console>:65\r\n",
       "avgDistancesO: org.apache.spark.rdd.RDD[((String, String), Double)] = MapPartitionsRDD[97] at mapValues at <console>:75\r\n",
       "minDistanceO: Double = 185.0\r\n",
       "maxDistanceO: Double = 3365.842672333445\r\n",
       "rangeO: Double = 1060.280890777815\r\n",
       "resultJobOptimized: org.apache.spark.rdd.RDD[(Int, String, Double)] = MapPartitionsRDD[104] at map at <console>:97\r\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.storage.StorageLevel._\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "val numClassesO = 3\n",
    "val numPartitions = sc.defaultParallelism\n",
    "val p = new HashPartitioner(numPartitions)\n",
    "\n",
    "val rddFlightsO = rawData.flatMap(FlightParser.parseFlightLine)\n",
    "    // (k,v) => (startingAirport, destinationAirport), (totalTravelDistance, flightDate, totalFare))\n",
    "    .map(flight => ((flight.startingAirport, flight.destinationAirport), \n",
    "                    (flight.totalTravelDistance, flight.flightMonth, flight.totalFare)))\n",
    "    .partitionBy(p)\n",
    "    .persist(MEMORY_AND_DISK_SER)\n",
    "    //.cache()\n",
    "\n",
    "val avgDistancesO = rddFlightsO\n",
    "    .aggregateByKey((0.0, 0))(\n",
    "    (acc, travelDistance) => (acc._1 + travelDistance._1, acc._2 + 1),\n",
    "    (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    "    )\n",
    "    // (k,v) => ((startingAirport, destinationAirport), avgDistance)\n",
    "    .mapValues { case (sumDistance, count) => sumDistance / count }\n",
    "    .persist(MEMORY_AND_DISK_SER)\n",
    "    //.cache()\n",
    "\n",
    "val (minDistanceO, maxDistanceO) = sc.broadcast(avgDistancesO\n",
    "    .aggregate((Double.MaxValue, Double.MinValue))(\n",
    "        (acc, avgDistance) => (Math.min(acc._1, avgDistance._2), Math.max(acc._2, avgDistance._2)),\n",
    "        (acc1, acc2) => (Math.min(acc1._1, acc2._1), Math.max(acc1._2, acc2._2))\n",
    "    )\n",
    ").value\n",
    "\n",
    "val rangeO = (maxDistanceO - minDistanceO) / numClassesO\n",
    "\n",
    "val resultJobOptimized = avgDistancesO\n",
    "    .mapValues {\n",
    "      case d if d < minDistanceNO + rangeNO => \"short\"\n",
    "      case d if d < minDistanceNO + (numClassesNO - 1) * rangeNO => \"medium\"\n",
    "      case _ => \"long\"\n",
    "    } // (k,v) => ((startingAirport, destinationAirport), classification)\n",
    "    .join(rddFlightsO)\n",
    "    .map { case (_, (classification, (_, month, totalFare))) => ((month, classification), (totalFare, 1)) }\n",
    "    .reduceByKey((acc, totalFare) => (acc._1 + totalFare._1, acc._2 + totalFare._2))\n",
    "    .map { case ((month, classification), (sumTotalFare, count)) => (month, classification, sumTotalFare / count) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f50cb4e7-dabe-4415-a9b8-77f778977fc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:12:35.202238Z",
     "start_time": "2025-01-29T15:12:20.261955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res18: Array[(Int, String, Double)] = Array((4,medium,330.325343533702), (10,short,259.3394174082933), (6,short,295.91989592541563), (8,long,461.90870554105663), (8,short,265.0115462401657), (10,long,412.0195542900455), (5,short,278.92276971784855), (11,medium,252.94729750177794), (6,medium,394.8199324698205), (4,long,478.75647959185164), (11,short,221.6570366598475), (9,short,256.2727750906376), (9,medium,293.2709198209645), (6,long,598.472643079619), (7,long,553.9726197049043), (5,long,534.0684905212262), (10,medium,295.50240627503814), (8,medium,323.21841789452395), (9,long,404.56801307297786), (7,short,290.5630628313932), (11,long,380.16898388672274), (7,medium,382.2243478418637), (5,medium,355.28277571564473), (4,short,304.4165681595479))\r\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultJobOptimized.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4eea600776b0d48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:12:45.125553Z",
     "start_time": "2025-01-29T15:12:35.205255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,910143)\r\n",
      "(1,842931)\r\n",
      "(2,792917)\r\n",
      "(3,1286794)\r\n",
      "(4,1097736)\r\n",
      "(5,1118030)\r\n",
      "(6,1017238)\r\n",
      "(7,950797)\r\n",
      "(8,1146810)\r\n",
      "(9,801914)\r\n",
      "(10,1044567)\r\n",
      "(11,1157260)\r\n"
     ]
    }
   ],
   "source": [
    "rddFlightsO.mapPartitionsWithIndex((index, iter) => Iterator((index, iter.size))).collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6113768893852a26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T13:58:36.931950Z",
     "start_time": "2025-01-29T13:58:35.555528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[(Int, String, Double)] = Array((11,short,222.00556413449337), (11,medium,254.4777731990232), (9,short,256.1263844227749), (10,short,259.3121553221906), (8,short,265.223595819795), (5,short,278.294492334017), (7,short,290.69249745467766), (9,medium,293.22341323056395), (10,medium,294.67349021286043), (6,short,296.05174334065117), (4,short,306.4335681610265), (8,medium,322.545062233574), (4,medium,331.0029844885145), (5,medium,354.65170115173936), (11,long,382.2554902106179), (7,medium,382.50144108560403), (6,medium,395.0468367755642), (9,long,405.4846256093826), (10,long,411.65239396939035), (8,long,462.04696232908344), (4,long,480.8690961538476), (5,long,533.1691284153799), (7,long,554.1280932802313), (6,long,598.8304861754746))\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultJobNotOptimized.sortBy(_._3, ascending = true).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "299c9bff9c1118d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T13:58:37.493434Z",
     "start_time": "2025-01-29T13:58:36.932958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: Array[(Int, String, Double)] = Array((11,short,222.00556413448837), (11,medium,254.4777731990195), (9,short,256.1263844227687), (10,short,259.3121553222084), (8,short,265.2235958197813), (5,short,278.29449233403625), (7,short,290.6924974546788), (9,medium,293.2234132305729), (10,medium,294.67349021288015), (6,short,296.0517433406561), (4,short,306.43356816102573), (8,medium,322.54506223357726), (4,medium,331.0029844885147), (5,medium,354.6517011517579), (11,long,382.25549021061255), (7,medium,382.5014410856088), (6,medium,395.0468367755694), (9,long,405.48462560937975), (10,long,411.6523939694063), (8,long,462.04696232906446), (4,long,480.8690961538468), (5,long,533.1691284153992), (7,long,554.1280932802221), (6,long,598.8304861754707))\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultJobOptimized.sortBy(_._3, ascending = true).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d05f2fa-3fd0-4c2c-ad56-272db2ca66cc",
   "metadata": {},
   "source": [
    "Questa cella, serve a liberare la memoria ed è stata eseguita solo quando necessario per motivi di *debugging*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc301cd1-e16f-4cdf-a813-87158ace9b42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T15:05:13.612282Z",
     "start_time": "2025-01-29T15:05:13.285762Z"
    }
   },
   "outputs": [],
   "source": [
    "sc.getPersistentRDDs.foreach(_._2.unpersist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e471e23c-b19c-4903-ad3f-89fc32ec610a",
   "metadata": {},
   "source": [
    "## Salvataggio dei risultati su file\n",
    "\n",
    "Innanzitutto si importa la modalità di salvataggio `org.apache.spark.sql.SaveMode`, poi si creano le cartelle dentro al quale mettere i risultati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37a687d9ab92eb8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-23T12:40:15.060137Z",
     "start_time": "2025-01-23T12:40:14.193201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SaveMode\r\n",
       "jobNotOptimized: String = ../../../../output/jobNotOptimized\r\n",
       "jobOptimized: String = ../../../../output/jobOptimized\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "val jobNotOptimized = \"../../../output/jobNotOptimized\"\n",
    "val jobOptimized = \"../../../output/jobOptimized\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16455e03e7403775",
   "metadata": {},
   "source": [
    "A partire dall'*RDD* si applica la funzione `coalesce(1)` per ridurre il partizionamento a una sola partizione in modo da ottenere un singolo file. Per salvare il file, bisogna usare delle API che appartengono all'oggetto *DataFrame*, ecco perché viene effettuato un *mapping* con la funzione `toDF()`. E' poi possibile richiamare l'interfaccia `write` per salvare il contenuto del *DataFrame* nel formato *.csv*, con il metodo di salvataggio `SaveMode.Overwrite` (se il file esiste già al momento del salvataggio, viene sovrascritto), all'interno della cartella precedentemente specificata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e94d29b-8e5f-46dc-8a01-efdf9125139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultJobNotOptimized\n",
    "  .coalesce(1)\n",
    "  .toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(jobNotOptimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3804c524-7d52-485c-a902-6505892fe7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultJobOptimized\n",
    "  .coalesce(1)\n",
    "  .toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(jobOptimized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
