{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529d658979e1113d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Obiettivo del progetto\n",
    "Dopo aver analizzato e compreso i dati, si vuole studiare meglio la correlazione riscontrata tra distanza e prezzo e si è individuato l'obiettivo del progetto. L'obiettivo è quello di verificare se c’è una stagionalità, nella quale i prezzi per alcuni mesi sono molto più elevati rispetto ad altri o se ci sono grandi variazioni di prezzo tra i diversi mesi rispetto alle diverse distanze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181bcbee1aae90f6",
   "metadata": {},
   "source": [
    "## Descrizione del job proposto\n",
    "Avendo a disposizione un solo file *.csv* si è pensato di usare un pattern di tipo *self-join*:\n",
    "\n",
    "-\t**Prima aggregazione**: aggregare per ogni combinazione di aeroporto di partenza e destinazione (*startingAeroport* e *destinationAeroport*) per ottenere la distanza media di viaggio (*totalTravelDistance*). A partire dalla distanza media generare una nuova colonna che indichi la fascia di distanza del volo (breve distanza, media distanza, lunga distanza);\n",
    "\n",
    "-\t**Join**: unire il dataset originale con il risultato ottenuto;\n",
    "\n",
    "-\t**Seconda aggregazione**: aggregare per fascia di distanza e mese (*flightDate*, da cui si ricava il mese) per ottenere per ciascuna combinazione il prezzo medio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae506c8e35674206",
   "metadata": {},
   "source": [
    "### Caricamento libreria Spark\n",
    "\n",
    "Per prima cosa, si deve importare la libreria spark per avviare una `spark-shell`; in seguito verrà mostrato il link tramite il quale è possibile accedere all'interfaccia utente di Spark."
   ]
  },
  {
   "cell_type": "code",
   "id": "672abbdbb9ccdca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:55:08.503500Z",
     "start_time": "2025-04-09T09:54:57.575879Z"
    }
   },
   "source": [
    "import org.apache.spark"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.5:4040\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1744192500706)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f69bc549d0a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// DO NOT EXECUTE - this is needed just to avoid showing errors in the following cells\n",
    "val sc = spark.SparkContext.getOrCreate()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Parser del file .csv\n",
    "\n",
    "Nella cella sottostante è implementata una `case class Flight` avente come parametri tutte le colonne(*) presenti nel file .csv descritti nel notebook [data-exploration.ipynb](./data-exploration.ipynb) e un `FlightParser` che consentente l'estrazione delle informazioni necessarie per popolare l'oggetto RDD di Spark.\n",
    "\n",
    "(*) nonostante vengano estratte tutte le colonne, per risolvere il `job` proposto verranno utilizzate solo alcune di esse."
   ],
   "id": "46ed8726b1c696c9"
  },
  {
   "cell_type": "code",
   "id": "285c89d7f3dc03d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:55:13.061887Z",
     "start_time": "2025-04-09T09:55:12.026527Z"
    }
   },
   "source": [
    "import java.text.SimpleDateFormat\n",
    "import java.util.Calendar\n",
    "\n",
    "/**\n",
    " * Flight case class.\n",
    " * @param legId The unique identifier for the flight.\n",
    " * @param searchMonth The month in which the search was conducted.\n",
    " * @param flightMonth The month in which the flight is scheduled.\n",
    " * @param startingAirport The airport code IATA for the departure location.\n",
    " * @param destinationAirport The airport code IATA for the arrival location.\n",
    " * @param fareBasisCode The fare basis code for the flight.\n",
    " * @param travelDuration The total duration of the flight.\n",
    " * @param elapsedDays The number of days elapsed.\n",
    " * @param isBasicEconomy Whether the flight is a basic economy flight.\n",
    " * @param isRefundable Whether the flight is refundable.\n",
    " * @param isNonStop Whether the flight is non-stop.\n",
    " * @param baseFare The base fare for the flight.\n",
    " * @param totalFare The total fare for the flight, including taxes and other fees.\n",
    " * @param seatsRemaining The number of seats remaining on the flight.\n",
    " * @param totalTravelDistance The total distance of the flight.\n",
    " * @param segmentsDepartureTimeEpochSeconds The departure time of each segment expressed as epoch seconds.\n",
    " * @param segmentsDepartureTimeRaw The departure time of each segment in raw format.\n",
    " * @param segmentsArrivalTimeEpochSeconds The arrival time of each segment expressed as epoch seconds.\n",
    " * @param segmentsArrivalTimeRaw The arrival time of each segment in raw format.\n",
    " * @param segmentsArrivalAirportCode The airport code IATA for the arrival location of each segment.\n",
    " * @param segmentsDepartureAirportCode The airport code IATA for the departure location of each segment.\n",
    " * @param segmentsAirlineName The name of the airline for each segment.\n",
    " * @param segmentsAirlineCode The code of the airline for each segment.\n",
    " * @param segmentsEquipmentDescription The description of the equipment for each segment.\n",
    " * @param segmentsDurationInSeconds The duration of each segment in seconds.\n",
    " * @param segmentsDistance The distance of each segment.\n",
    " * @param segmentsCabinCode The cabin code for each segment.\n",
    " */\n",
    "case class Flight(\n",
    "    legId: String,\n",
    "    searchMonth: Int,\n",
    "    flightMonth: Int,\n",
    "    startingAirport: String,\n",
    "    destinationAirport: String,\n",
    "    fareBasisCode: String,\n",
    "    travelDuration: String,\n",
    "    elapsedDays: Int,\n",
    "    isBasicEconomy: Boolean,\n",
    "    isRefundable: Boolean,\n",
    "    isNonStop: Boolean,\n",
    "    baseFare: Double,\n",
    "    totalFare: Double,\n",
    "    seatsRemaining: Int,\n",
    "    totalTravelDistance: Double,\n",
    "    segmentsDepartureTimeEpochSeconds: String,\n",
    "    segmentsDepartureTimeRaw: String,\n",
    "    segmentsArrivalTimeEpochSeconds: String,\n",
    "    segmentsArrivalTimeRaw: String,\n",
    "    segmentsArrivalAirportCode: String,\n",
    "    segmentsDepartureAirportCode: String,\n",
    "    segmentsAirlineName: String,\n",
    "    segmentsAirlineCode: String,\n",
    "    segmentsEquipmentDescription: String,\n",
    "    segmentsDurationInSeconds: String,\n",
    "    segmentsDistance: String,\n",
    "    segmentsCabinCode: String\n",
    ") extends Serializable\n",
    "\n",
    "/**\n",
    " * Flight parser.\n",
    " */\n",
    "object FlightParser extends Serializable {\n",
    "\n",
    "  val comma = \",\"\n",
    "\n",
    "  /**\n",
    "   * Convert from date (String) to month (Int).\n",
    "   * @param dateString the date\n",
    "   * @return the month\n",
    "   */\n",
    "  def monthFromDate(dateString: String): Int = {\n",
    "    val sdf = new SimpleDateFormat(\"yyyy-MM-dd\")\n",
    "    val date = sdf.parse(dateString.trim)\n",
    "    val cal = Calendar.getInstance()\n",
    "    cal.setTime(date)\n",
    "    cal.get(Calendar.MONTH) + 1\n",
    "  }\n",
    "\n",
    "  /**\n",
    "   * Function to parse flights records.\n",
    "   * @param line that has to be parsed\n",
    "   * @return Flight object, None in case of input errors\n",
    "   */\n",
    "  def parseFlightLine(line: String): Option[Flight] = {\n",
    "    try {\n",
    "      val columns = line.split(comma)\n",
    "      Some(\n",
    "        Flight(\n",
    "          legId = columns(0).trim,\n",
    "          searchMonth = monthFromDate(columns(1)),\n",
    "          flightMonth = monthFromDate(columns(2)),\n",
    "          startingAirport = columns(3).trim,\n",
    "          destinationAirport = columns(4).trim,\n",
    "          fareBasisCode = columns(5).trim,\n",
    "          travelDuration = columns(6).trim,\n",
    "          elapsedDays = columns(7).trim.toInt,\n",
    "          isBasicEconomy = columns(8).trim.toBoolean,\n",
    "          isRefundable = columns(9).trim.toBoolean,\n",
    "          isNonStop = columns(10).trim.toBoolean,\n",
    "          baseFare = columns(11).trim.toDouble,\n",
    "          totalFare = columns(12).trim.toDouble,\n",
    "          seatsRemaining = columns(13).trim.toInt,\n",
    "          totalTravelDistance = columns(14).trim.toDouble,\n",
    "          segmentsDepartureTimeEpochSeconds = columns(15).trim,\n",
    "          segmentsDepartureTimeRaw = columns(16).trim,\n",
    "          segmentsArrivalTimeEpochSeconds = columns(17).trim,\n",
    "          segmentsArrivalTimeRaw = columns(18).trim,\n",
    "          segmentsArrivalAirportCode = columns(19).trim,\n",
    "          segmentsDepartureAirportCode = columns(20).trim,\n",
    "          segmentsAirlineName = columns(21).trim,\n",
    "          segmentsAirlineCode = columns(22).trim,\n",
    "          segmentsEquipmentDescription = columns(23).trim,\n",
    "          segmentsDurationInSeconds = columns(24).trim,\n",
    "          segmentsDistance = columns(25).trim,\n",
    "          segmentsCabinCode = columns(26).trim\n",
    "        )\n",
    "      )\n",
    "    } catch {\n",
    "      case e: Exception =>\n",
    "//        println(s\"Error during parsing of the line '$line': ${e.getMessage}\")\n",
    "        None\n",
    "    }\n",
    "  }\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.text.SimpleDateFormat\r\n",
       "import java.util.Calendar\r\n",
       "defined class Flight\r\n",
       "defined object FlightParser\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "d195400e1cea1b18",
   "metadata": {},
   "source": [
    "### Caricamento dei dati\n",
    "\n",
    "Con la seguente cella si effettua il caricamento del file *itineraries-sample\\<N\\>.csv*, dove con N si intende la percentuale di dati campionati dal file originale di 31,09 GB.\n",
    "\n",
    "I file disponibili sono scaricabili dalla cartella [Datasets](https://liveunibo-my.sharepoint.com/:f:/g/personal/giulia_nardicchia_studio_unibo_it/Ei2686kRO3JFrY-4LnImGpwBtge9FRErDnIgvT2h2QB-Pg?e=VrufWl) su OneDrive e hanno percentuale: `02`, `16` e `33`."
   ]
  },
  {
   "cell_type": "code",
   "id": "a374b84c2b34803a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:55:16.303189Z",
     "start_time": "2025-04-09T09:55:15.739782Z"
    }
   },
   "source": [
    "val datasetsPath = \"../../../datasets/\"\n",
    "val fileName = \"itineraries-sample02.csv\"\n",
    "\n",
    "val rawData = sc.textFile(datasetsPath + fileName)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasetsPath: String = ../../../datasets/\r\n",
       "fileName: String = itineraries-sample02.csv\r\n",
       "rawData: org.apache.spark.rdd.RDD[String] = ../../../datasets/itineraries-sample02.csv MapPartitionsRDD[1] at textFile at <console>:30\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "55ad3c19d0de51fa",
   "metadata": {},
   "source": [
    "Trasformazione di un RDD composto da dati grezzi (*rawData*) in un RDD di oggetti `Flight`. La funzione `FlightParser.parseFlightLine` analizza ogni riga. `flatMap` appiattisce i risultati, scartando automaticamente le righe non valide."
   ]
  },
  {
   "cell_type": "code",
   "id": "ba8c35cd3020a7a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:55:19.584589Z",
     "start_time": "2025-04-09T09:55:19.077676Z"
    }
   },
   "source": [
    "val rddFlights = rawData.flatMap(FlightParser.parseFlightLine)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddFlights: org.apache.spark.rdd.RDD[Flight] = MapPartitionsRDD[2] at flatMap at <console>:28\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "adcd44f9230e1434",
   "metadata": {},
   "source": [
    "Per verificare che non ci siano stati problemi di *parsing*, con la cella seguente si vuole eseguire un'azione. La funzione `count()` calcola il numero di righe valide."
   ]
  },
  {
   "cell_type": "code",
   "id": "411a20cf90856722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:55:31.307056Z",
     "start_time": "2025-04-09T09:55:21.868659Z"
    }
   },
   "source": [
    "rddFlights.count()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Long = 1622775\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "1500dc766c51eab",
   "metadata": {},
   "source": [
    "### Prima aggregazione\n",
    "\n",
    "Innanzitutto si utilizza `map` per ridurre i dati da manipolare, quindi vengono scartate tutte le colonne che non servono a svolgere il job proposto e per trasformare i dati di tipo (chiave, valore). Si vuole aggregare per ogni combinazione di aeroporto di partenza e destinazione (*startingAeroport* e *destinationAeroport*) per ottenere la distanza media di viaggio (*totalTravelDistance*). Per fare questo si utilizza la funzione `aggregateByKey()` passando come argomenti: (0.0, 0), i due valori iniziali, una funzione *combining* (*map*) per indicare come i singoli valori sono combinati con l'accumulatore e una funzione di *merging* (*reduce*) per indicare come gli accumulatori per le differenti partizioni devono essere uniti. Come funzioni di *combining* sono state passate due funzioni **associative** e **commutative** quali la somma e il conteggio, una volta ottenuti questi risultati si effettua il rapporto per calcolare la media delle distanze."
   ]
  },
  {
   "cell_type": "code",
   "id": "fb3928b8618a571d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:55:41.616443Z",
     "start_time": "2025-04-09T09:55:41.038243Z"
    }
   },
   "source": [
    "val avgDistances = rddFlights\n",
    "  .map(flight => ((flight.startingAirport, flight.destinationAirport), flight.totalTravelDistance))\n",
    "  .aggregateByKey((0.0, 0))(\n",
    "    (acc, travelDistance) => (acc._1 + travelDistance, acc._2 + 1),\n",
    "    (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    "  )\n",
    "  .mapValues { case (sumDistance, count) => sumDistance / count }"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgDistances: org.apache.spark.rdd.RDD[((String, String), Double)] = MapPartitionsRDD[5] at mapValues at <console>:33\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "22f52f39-34a4-4c7c-832f-f40b96ce0570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:56:31.982796Z",
     "start_time": "2025-04-09T09:56:30.570522Z"
    }
   },
   "source": "avgDistances.take(10)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[((String, String), Double)] = Array(((IAD,LGA),999.9239723535832), ((MIA,OAK),3314.2077417173764), ((EWR,LAX),2576.270231126667), ((DFW,CLT),1309.5679179044018), ((BOS,ATL),1040.5552817985833), ((EWR,PHL),1207.7278366502971), ((ORD,IAD),905.9191603394372), ((OAK,BOS),3169.411759118541), ((MIA,CLT),1221.4948882998865), ((ORD,BOS),931.8038178038178))\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "d58d5b9e7bd8c078",
   "metadata": {},
   "source": [
    "A partire dalla distanza media, si intende generare una nuova colonna che specifichi la fascia di distanza del volo, distinguendo tra breve distanza, media distanza e lunga distanza.\n",
    "\n",
    "Poiché usare valori numerici *hard coded* rappresenta una *bad practice*, si è deciso di utilizzare il minimo, il massimo e il numero di classi per calcolare dinamicamente l'intervallo delle fasce di distanza. Tuttavia, si è consapevoli che, nel caso in cui il codice venisse riutilizzato su un diverso dataset, gli intervalli calcolati potrebbero perdere di significato. Le distanze, infatti, potrebbero variare significativamente, facendo sì che uno stesso percorso, classificato come breve in un dataset, risulti invece di media o lunga distanza in un altro. Rendendo così il confronto tra i due dataset non più fattibile.\n",
    "\n",
    "Per ottenere il minimo e il massimo su `avgDistances` appena calcolato, si utilizza la funzione `aggregate()` specificando come valore iniziale per il minimo `Double.MaxValue` mentre per il massimo `Double.MinValue`. Per confrontare, invece, i valori tra di loro si utilizza il package `scala.math` con la funzione `math.min()` e `math.max()`."
   ]
  },
  {
   "cell_type": "code",
   "id": "66305cd03e86b23d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:56:37.687192Z",
     "start_time": "2025-04-09T09:56:37.329050Z"
    }
   },
   "source": [
    "val (minDistance, maxDistance) = avgDistances\n",
    "    .aggregate((Double.MaxValue, Double.MinValue))(\n",
    "        (acc, value) => (math.min(acc._1, value._2), math.max(acc._2, value._2)),\n",
    "        (acc1, acc2) => (math.min(acc1._1, acc2._1), math.max(acc1._2, acc2._2))\n",
    "    )"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minDistance: Double = 393.6039441248973\r\n",
       "maxDistance: Double = 3442.6743515850144\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "1bef001288d0d29f",
   "metadata": {},
   "source": [
    "Le distanze sono state suddivise in tre fasce: breve, media e lunga (*numClasses = 3*).\n",
    "\n",
    "Per calcolare il *range* in maniera equidistante si è calcolato la differenza tra la distanza massima e minima e il rapporto tra il valore calcolato e il numero di classi."
   ]
  },
  {
   "cell_type": "code",
   "id": "d093654c6806b601",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:56:40.634796Z",
     "start_time": "2025-04-09T09:56:40.442123Z"
    }
   },
   "source": [
    "val numClasses = 3\n",
    "val range = (maxDistance - minDistance) / numClasses"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numClasses: Int = 3\r\n",
       "range: Double = 1016.3568024867058\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "998c5f70d0a66232",
   "metadata": {},
   "source": [
    "Per calcolare l'intervallo sono stati adottati i seguenti limiti:\n",
    "- **Breve**: se la distanza media è inferiore a *minimo + intervallo*;\n",
    "- **Media**: se la distanza media è compresa tra *minimo + intervallo; minimo + 2 * intervallo*;\n",
    "- **Lunga**: se la distanza media è superiore a *minimo + (numero classi - 1) * intervallo*."
   ]
  },
  {
   "cell_type": "code",
   "id": "7d520d4cff3132ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:56:43.571190Z",
     "start_time": "2025-04-09T09:56:43.321968Z"
    }
   },
   "source": [
    "val classifiedDistances = avgDistances.mapValues {\n",
    "  case d if d < minDistance + range => \"short\"\n",
    "  case d if d < minDistance + 2 * range => \"medium\"\n",
    "  case _ => \"long\"\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifiedDistances: org.apache.spark.rdd.RDD[((String, String), String)] = MapPartitionsRDD[6] at mapValues at <console>:29\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:56:51.423708Z",
     "start_time": "2025-04-09T09:56:49.699758Z"
    }
   },
   "cell_type": "code",
   "source": "classifiedDistances.take(10)",
   "id": "e0a3b30a51c76229",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[((String, String), String)] = Array(((IAD,LGA),short), ((MIA,OAK),long), ((EWR,LAX),long), ((DFW,CLT),short), ((BOS,ATL),short), ((EWR,PHL),short), ((ORD,IAD),short), ((OAK,BOS),long), ((MIA,CLT),short), ((ORD,BOS),short))\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "c24023d49bd4355e",
   "metadata": {},
   "source": [
    "### Join + Seconda aggregazione\n",
    "\n",
    "Si unisce il dataset originale con il risultato ottenuto, aggregando poi per fascia di distanza e mese (*flightDate*, da cui si ricava il mese) si ottiene per ciascuna combinazione il prezzo medio.\n",
    "\n",
    "Per fare ciò, si utilizza la funzione `join` per unire i due dataset, successivamente si effettua un `map` per ottenere una nuova coppia chiave-valore, in cui la chiave è composta dal mese e dalla classificazione della distanza, mentre il valore è composto dal prezzo totale e il conteggio. Si effettua un `reduceByKey` per aggregare i valori in base alla chiave e calcolare la somma totale e il conteggio. Alla `reduceByKey` è stata aggiunta una funzione di arrotondamento per evitare errori dovuti alla precisione dei numeri in virgola mobile. Infine, si effettua un `map` per ottenere il risultato finale."
   ]
  },
  {
   "cell_type": "code",
   "id": "7c291f167c156e59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:57:07.212559Z",
     "start_time": "2025-04-09T09:57:06.930943Z"
    }
   },
   "source": [
    "val resultJob = rddFlights\n",
    "  .map(flight => ((flight.startingAirport, flight.destinationAirport), (flight.flightMonth, flight.totalFare)))\n",
    "  .join(classifiedDistances)\n",
    "  .map {\n",
    "    case (_, ((flightMonth, totalFare), classification)) => ((flightMonth, classification), (totalFare, 1))\n",
    "  }\n",
    "  .reduceByKey((acc, totalFare) =>\n",
    "    (BigDecimal(acc._1 + totalFare._1).setScale(2, BigDecimal.RoundingMode.HALF_UP).toDouble,\n",
    "      acc._2 + totalFare._2))\n",
    "  .map {\n",
    "    case ((flightMonth, classification), (sumTotalFare, count)) => (flightMonth, classification, sumTotalFare / count)\n",
    "  }"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resultJob: org.apache.spark.rdd.RDD[(Int, String, Double)] = MapPartitionsRDD[13] at map at <console>:37\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "28398214304d1cd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:57:22.376467Z",
     "start_time": "2025-04-09T09:57:16.732594Z"
    }
   },
   "source": [
    "resultJob.collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Array[(Int, String, Double)] = Array((9,medium,318.3760535316619), (10,short,272.90626195986323), (8,short,285.3570154260343), (5,short,294.7269176582328), (11,medium,278.41263953314376), (6,medium,432.32829361007856), (6,long,602.9717814924518), (4,long,477.23622320768663), (8,medium,347.8695456922884), (7,short,314.0343191116306), (10,long,419.78446130428574), (4,short,310.6395352791159), (4,medium,347.80798695523276), (10,medium,319.77589381861407), (5,long,539.7802670253963), (8,long,467.07980860504125), (9,long,409.2186325749457), (11,short,232.2905439102314), (7,medium,411.2507502278304), (5,medium,387.795496742671), (9,short,268.8361475656684), (7,long,557.3582296212151), (6,short,315.64908727581553), (11,long,392.619358541526))\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Salvataggio dei risultati su file\n",
    "\n",
    "Innanzitutto si importa la modalità di salvataggio `org.apache.spark.sql.SaveMode`, poi si creano le cartelle dentro al quale mettere i risultati."
   ],
   "id": "7b2dfe407abf88b1"
  },
  {
   "cell_type": "code",
   "id": "37a687d9ab92eb8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:57:29.965478Z",
     "start_time": "2025-04-09T09:57:29.776509Z"
    }
   },
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "val job = \"../../../output/job\""
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SaveMode\r\n",
       "job: String = ../../../output/job\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "16455e03e7403775",
   "metadata": {},
   "source": [
    "A partire dall'*RDD* si applica la funzione `coalesce(1)` per ridurre il partizionamento a una sola partizione in modo da ottenere un singolo file. Per salvare il file, bisogna usare delle API che appartengono all'oggetto *DataFrame*, ecco perché viene effettuato un *mapping* con la funzione `toDF()`. E' poi possibile richiamare l'interfaccia `write` per salvare il contenuto del *DataFrame* nel formato *.csv*, con il metodo di salvataggio `SaveMode.Overwrite` (se il file esiste già al momento del salvataggio, viene sovrascritto), all'interno della cartella precedentemente specificata."
   ]
  },
  {
   "cell_type": "code",
   "id": "0e94d29b-8e5f-46dc-8a01-efdf9125139e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:57:47.447867Z",
     "start_time": "2025-04-09T09:57:43.907833Z"
    }
   },
   "source": [
    "resultJob\n",
    "  .coalesce(1)\n",
    "  .toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(job)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Job Not Optimized\n",
    "L'ordine con cui sono state eseguite alcune operazioni, è migliorabile e quindi a partire dal codice scritto nelle precedenti celle, si è proceduto a *\"rifattorizzare\"* l'implementazione. Di seguito il *job* proposto non ottimizzato, in cui il nome delle variabili usate è il medesimo con l'aggiunta del suffisso NO (*Not Optimized*).\n",
    "\n",
    "Differenze:\n",
    "   - Innanzitutto, è stato migliorato il *mapping* iniziale, così da evitare di eseguire un ulteriore *mapping* in seguito per effettuare il *join*. In pratica vengono estratte le colonne a cui si è interessati subito (*totalTravelDistance, flightMonth, totalFare*) e viene usato quello come dataset di partenza.\n",
    "\n",
    "   - Il *join*, adesso viene eseguito sul risultato ottenuto con la classificazione, al contrario della modalità precedente."
   ],
   "id": "facae0ec0135dfc0"
  },
  {
   "cell_type": "code",
   "id": "a9150b0fb9cee78f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:58:45.107398Z",
     "start_time": "2025-04-09T09:58:28.434619Z"
    }
   },
   "source": [
    "val numClassesNO = 3\n",
    "val jobNotOptimized = \"../../../output/jobNotOptimized\"\n",
    "\n",
    "val rddFlightsNO = rawData.flatMap(FlightParser.parseFlightLine)\n",
    "    // (k,v) => (startingAirport, destinationAirport), (totalTravelDistance, flightDate, totalFare))\n",
    "    .map(flight => ((flight.startingAirport, flight.destinationAirport),\n",
    "                    (flight.totalTravelDistance, flight.flightMonth, flight.totalFare)))\n",
    "\n",
    "val avgDistancesNO = rddFlightsNO\n",
    "    .aggregateByKey((0.0, 0))(\n",
    "        (acc, travelDistance) => (acc._1 + travelDistance._1, acc._2 + 1),\n",
    "        (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    "    )\n",
    "    // (k,v) => ((startingAirport, destinationAirport), avgDistance)\n",
    "    .mapValues { case (sumDistance, count) => sumDistance / count }\n",
    "\n",
    "val (minDistanceNO, maxDistanceNO) = avgDistancesNO\n",
    "    .aggregate((Double.MaxValue, Double.MinValue))(\n",
    "        (acc, avgDistance) => (math.min(acc._1, avgDistance._2), math.max(acc._2, avgDistance._2)),\n",
    "        (acc1, acc2) => (math.min(acc1._1, acc2._1), math.max(acc1._2, acc2._2))\n",
    "    )\n",
    "\n",
    "val rangeNO = (maxDistanceNO - minDistanceNO) / numClassesNO\n",
    "\n",
    "val resultJobNotOptimized = avgDistancesNO\n",
    "    .mapValues {\n",
    "      case d if d < minDistanceNO + rangeNO => \"short\"\n",
    "      case d if d < minDistanceNO + 2 * rangeNO => \"medium\"\n",
    "      case _ => \"long\"\n",
    "    } // (k,v) => ((startingAirport, destinationAirport), classification)\n",
    "    .join(rddFlightsNO)\n",
    "    .map { case (_, (classification, (_, month, totalFare))) => ((month, classification), (totalFare, 1)) }\n",
    "    .reduceByKey((acc, totalFare) =>\n",
    "      (BigDecimal(acc._1 + totalFare._1).setScale(2, BigDecimal.RoundingMode.HALF_UP).toDouble,\n",
    "        acc._2 + totalFare._2))\n",
    "    .map { case ((month, classification), (sumTotalFare : Double, count: Int)) => (month, classification, sumTotalFare / count) }\n",
    "    .coalesce(1)\n",
    "    .toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(jobNotOptimized)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numClassesNO: Int = 3\r\n",
       "jobNotOptimized: String = ../../../output/jobNotOptimized\r\n",
       "rddFlightsNO: org.apache.spark.rdd.RDD[((String, String), (Double, Int, Double))] = MapPartitionsRDD[19] at map at <console>:34\r\n",
       "avgDistancesNO: org.apache.spark.rdd.RDD[((String, String), Double)] = MapPartitionsRDD[21] at mapValues at <console>:43\r\n",
       "minDistanceNO: Double = 393.6039441248973\r\n",
       "maxDistanceNO: Double = 3442.6743515850144\r\n",
       "rangeNO: Double = 1016.3568024867058\r\n",
       "resultJobNotOptimized: Unit = ()\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "6047e0c20ab911f7",
   "metadata": {},
   "source": [
    "## Considerazioni sulle ottimizzazioni\n",
    "\n",
    "Si vogliono fare alcune considerazioni riguardo le ottimizzazioni che sono disponibili in Spark. Durante il corso sono state analizzate diverse tecniche per migliorare le prestazioni. Ogni tecnica presenta vantaggi e svantaggi che devono essere valutati in base al contesto specifico."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Cache / Persist\n",
    "\n",
    "Innanzitutto, sono state considerate le tecniche per mantenere in memoria i dati tramite `cache` o `persist`. Le operazioni di caching o persistenza sono utili quando i dati vengono riutilizzati più volte. L'utilizzo di questo meccanismo riduce il costo computazionale, evitando il ricalcolo delle stesse operazioni multiple volte e migliorando le prestazioni in caso di ripetuti accessi agli stessi dati. Tuttavia, bisogna evitare di utilizzare `cache` o `persist` su dataset di grandi dimensioni se non si prevede che vengano riutilizzati più volte, in quanto ciò potrebbe causare un uso eccessivo della memoria, rallentando l'intero sistema o addirittura facendo fallire il job.\n",
    "\n",
    "Differenze tra i vari metodi:\n",
    "- **cache**: di default usa il livello di memorizzazione `MEMORY_ONLY`. I dati vengono memorizzati completamente in memoria e non vengono scritti su disco. Questo è utile quando l'*RDD* è piccolo e può essere completamente caricato in memoria. Se i dati non possono essere memorizzati in memoria si avrà un errore di `OutOfMemoryError`.\n",
    "- **persist**: consente di specificare il livello di memorizzazione desiderato, tra cui: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`, `MEMORY_AND_DISK_SER`, `DISK_ONLY`. Tra questi il metodo più interessante è `persist(MEMORY_AND_DISK_SER)` che usa la memoria se disponibile, altrimenti scrive su disco in formato serializzato. I dati vengono memorizzati in modo più compatto grazie alla serializzazione, ma introduce una latenza aggiuntiva per effettuare la deserializzazione. `MEMORY_AND_DISK_SER` è utile quando i dati sono troppo grandi per rimanere completamente in memoria, o se l'elaborazione dei dati è costosa e il riutilizzo dei dati è frequente."
   ],
   "id": "36fe84cd2fbb694c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### PartitionBy / Repartition / Coalesce\n",
    "Le operazioni di partizionamento sono utili quando si desidera ottimizzare la distribuzione dei dati su più partizioni. L'ottimizzazione delle partizioni può ridurre significativamente il costo delle operazioni di shuffle, che sono spesso le più costose in termini di tempo di esecuzione. Una distribuzione non uniforme dei dati su partizioni può causare squilibri nel carico di lavoro tra i nodi, portando a una scarsa performance.\n",
    "\n",
    "Differenze tra i vari metodi:\n",
    "- **partitionBy**: questo metodo partiziona i dati in base alla chiave, ed è particolarmente utile quando si effettuano operazioni come `join()` su chiavi. Utilizzare un `HashPartitioner()` aiuta a ridurre il costo dello shuffle, perché i dati con la stessa chiave finiranno nella stessa partizione. Inoltre, evita lo shuffle tra nodi, se l'*RDD* è già partizionato in modo compatibile.\n",
    "- **repartition**: è un'operazione costosa che implica un completo reshuffling dei dati, cambiando il numero di partizioni. È utile quando si vuole migliorare la distribuzione dei dati o quando si ha un numero di partizioni iniziale troppo basso. Tuttavia, se non usata con attenzione, può risultare inefficiente per grandi set di dati.\n",
    "- **coalesce**: è meno costoso rispetto a `repartition()`, poiché cerca di ridurre il numero di partizioni combinando le partizioni esistenti senza fare uno shuffle completo. È utile quando si vuole ridurre il numero di partizioni, ad esempio, prima di scrivere i dati su disco. Tuttavia, non può aumentare il numero di partizioni."
   ],
   "id": "1611822c6c016f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Broadcast Variables\n",
    "\n",
    "Le **broadcast variables** sono utili per inviare copie di dati *read-only* a tutti i nodi in modo efficiente, senza la necessità di ricalcolarli ripetutamente durante ogni task. Sono adatte per dati immutabili che sono letti da tutti i task. L'uso di broadcast riduce il traffico di rete e migliora le performance. Tuttavia, bisogna evitare di utilizzare le variabili broadcast con dataset troppo grandi, poiché i dati verranno copiati su tutti i nodi e potrebbero sovraccaricare la memoria di ciascun nodo, riducendo le prestazioni complessive.\n"
   ],
   "id": "7d9c9a4126009324"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Job Optimized\n",
    "\n",
    "Una volta capite quali tecniche di ottimizzazione adottare per migliorare le performance, il *main job* è stato modificato ottenendo così il seguente codice.\n",
    "\n",
    "Differenze rispetto al *job* proposto non ottimizzato:\n",
    "- **PartitionBy**: si è deciso di utilizzare il meccanismo di partizionamento `partitionBy` passando in ingresso un `HashPartitioner` in modo tale da assicurare che i dati con la stessa chiave finiscano nella stessa partizione. Questo aiuta a ridurre il costo dello shuffle durante operazioni costose come `aggregateByKey` e `join` subito dopo. Inoltre, per rendere il `join` più efficiente si dovrebbe utilizzare lo stesso partizionamento per entrambi gli *RDD* (su cui verrà applicata l'operazione di `join`) ma dato che il dataset è lo stesso, è stato utilizzato il partizionamento una sola volta all'inizio.\n",
    "- **Cache / Persist**: si è deciso di utilizzare il meccanismo di persistenza con il livello di memorizzazione `MEMORY_AND_DISK_SER` per mantenere in memoria i dati, evitando di rieseguire il `map` e il `partitionBy` per quanto riguarda `rddFlights`. Siccome in questo punto la quantità dei dati non è poca si è scelto di non fare memorizzazione `MEMORY_ONLY`. Anche per quanto riguarda `avgDistances` si è deciso di utilizzare `MEMORY_AND_DISK_SER`, per evitare che venga rieseguita l'operazione di `aggregateByKey`.\n",
    "- **Broadcast Variables**: si è deciso di utilizzare il meccanismo di broadcast per le variabili `minDistance` e il `range` per la classificazione delle distanze, in quanto i valori sono i medesimi per tutti, così da non doverli ricalcolare per ogni task."
   ],
   "id": "8924e8be9e9aa1fe"
  },
  {
   "cell_type": "code",
   "id": "e64457ebabde5cf3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T10:04:48.747815Z",
     "start_time": "2025-04-09T10:04:40.432913Z"
    }
   },
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.storage.StorageLevel._\n",
    "import org.apache.spark.HashPartitioner\n",
    "\n",
    "val numPartitions = sc.defaultParallelism\n",
    "val p = new HashPartitioner(numPartitions)\n",
    "val jobOptimized = \"../../../output/jobOptimized\"\n",
    "val numClassesO = 3\n",
    "\n",
    "val rddFlightsO = rawData.flatMap(FlightParser.parseFlightLine)\n",
    "  // (k,v) => (startingAirport, destinationAirport), (totalTravelDistance, flightDate, totalFare))\n",
    "  .map(flight => ((flight.startingAirport, flight.destinationAirport),\n",
    "    (flight.totalTravelDistance, flight.flightMonth, flight.totalFare)))\n",
    "  .partitionBy(p)\n",
    "  .persist(MEMORY_AND_DISK_SER)\n",
    "\n",
    "val avgDistancesO = rddFlightsO\n",
    "  .aggregateByKey((0.0, 0))(\n",
    "    (acc, travelDistance) => (acc._1 + travelDistance._1, acc._2 + 1),\n",
    "    (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    "  )\n",
    "  // (k,v) => ((startingAirport, destinationAirport), avgDistance)\n",
    "  .mapValues { case (sumDistance, count) => sumDistance / count }\n",
    "  .persist(MEMORY_AND_DISK_SER)\n",
    "\n",
    "val (minDistanceO, maxDistanceO) = avgDistancesO\n",
    "  .aggregate((Double.MaxValue, Double.MinValue))(\n",
    "    (acc, avgDistance) => (Math.min(acc._1, avgDistance._2), Math.max(acc._2, avgDistance._2)),\n",
    "    (acc1, acc2) => (Math.min(acc1._1, acc2._1), Math.max(acc1._2, acc2._2))\n",
    "  )\n",
    "\n",
    "val broadcastStats = sc.broadcast((minDistanceO, (maxDistanceO - minDistanceO) / numClassesO))\n",
    "\n",
    "val resultJobOptimized = avgDistancesO\n",
    "  .mapValues { d =>\n",
    "    val (minDist, range) = broadcastStats.value\n",
    "    if (d < minDist + range) \"short\"\n",
    "    else if (d < minDist + 2 * range) \"medium\"\n",
    "    else \"long\"\n",
    "  }\n",
    "  // (k,v) => ((startingAirport, destinationAirport), classification)\n",
    "  .join(rddFlightsO)\n",
    "  .map { case (_, (classification, (_, month, totalFare))) => ((month, classification), (totalFare, 1)) }\n",
    "  .reduceByKey((acc, totalFare) =>\n",
    "    (BigDecimal(acc._1 + totalFare._1).setScale(2, BigDecimal.RoundingMode.HALF_UP).toDouble, acc._2 + totalFare._2))\n",
    "  .map { case ((month, classification), (sumTotalFare, count)) => (month, classification, sumTotalFare / count) }\n",
    "  .coalesce(1)\n",
    "  .toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(jobOptimized)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SaveMode\r\n",
       "import org.apache.spark.storage.StorageLevel._\r\n",
       "import org.apache.spark.HashPartitioner\r\n",
       "numPartitions: Int = 8\r\n",
       "p: org.apache.spark.HashPartitioner = org.apache.spark.HashPartitioner@8\r\n",
       "jobOptimized: String = ../../../output/jobOptimized\r\n",
       "numClassesO: Int = 3\r\n",
       "rddFlightsO: org.apache.spark.rdd.RDD[((String, String), (Double, Int, Double))] = ShuffledRDD[35] at partitionBy at <console>:44\r\n",
       "avgDistancesO: org.apache.spark.rdd.RDD[((String, String), Double)] = MapPartitionsRDD[37] at mapValues at <console>:53\r\n",
       "minDistanceO: Double = 393.6039441248973\r\n",
       "maxDistanceO: Double = 3442.6743515850144\r\n",
       "broadcastStats: org.apache.spark.broadcast.Broadcast[(Double, Double)] = Broadcast(18)\r\n",
       "resultJobOptimized: Unit = ()\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "2d05f2fa-3fd0-4c2c-ad56-272db2ca66cc",
   "metadata": {},
   "source": [
    "Questa cella, serve a liberare la memoria ed è stata eseguita solo quando necessario per motivi di *debugging*."
   ]
  },
  {
   "cell_type": "code",
   "id": "fc301cd1-e16f-4cdf-a813-87158ace9b42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T19:07:21.877113Z",
     "start_time": "2025-03-18T19:07:21.556158Z"
    }
   },
   "source": "// sc.getPersistentRDDs.foreach(_._2.unpersist())",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
