{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45909d062047387d",
   "metadata": {},
   "source": [
    "# Obiettivo del progetto\n",
    "Dopo aver analizzato e compreso i dati, si vuole studiare meglio la correlazione riscontrata tra distanza e prezzo e si è individuato l'obiettivo del progetto. L'obiettivo è quello di verificare se c’è una stagionalità, nella quale i prezzi per alcuni mesi sono molto più elevati rispetto ad altri o se ci sono grandi variazioni di prezzo tra i diversi mesi rispetto alle diverse distanze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5699d07ef8e27",
   "metadata": {},
   "source": [
    "## Descrizione del job proposto\n",
    "Avendo a disposizione un solo file *.csv* si è pensato si usare un pattern di tipo *self-join*:\n",
    "\n",
    "-\t**Prima aggregazione**: aggregare per ogni combinazione di aeroporto di partenza e destinazione (*startingAeroport* e *destinationAeroport*) per ottenere la distanza media di viaggio (*totalTravelDistance*). A partire dalla distanza media generare una nuova colonna che indichi la fascia di distanza del volo (breve distanza, media distanza, lunga distanza);\n",
    "\n",
    "-\t**Join**: unire il dataset originale con il risultato ottenuto;\n",
    "\n",
    "-\t**Seconda aggregazione**: aggregare per fascia di distanza e mese (*flightDate*, da cui si ricava il mese) per ottenere per ciascuna combinazione il prezzo medio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae506c8e35674206",
   "metadata": {},
   "source": [
    "### Caricamento libreria Spark\n",
    "\n",
    "Per prima cosa, si deve importare la libreria spark per avviare una `spark-shell`; in seguito verrà mostrato il link tramite il quale è possibile accedere all'interfaccia utente di Spark."
   ]
  },
  {
   "cell_type": "code",
   "id": "fb23d8dd5e72ce9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T14:50:08.456469Z",
     "start_time": "2025-01-16T14:50:07.687625Z"
    }
   },
   "source": [
    "import org.apache.spark"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f69bc549d0a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "// DO NOT EXECUTE - this is needed just to avoid showing errors in the following cells\n",
    "val sc = spark.SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c45630c708235f",
   "metadata": {},
   "source": [
    "### Parser del file .csv\n",
    "\n",
    "La cella sottostante implementa un *parser* per il file .csv descritto nel notebook [data-exploration.ipynb](./data-exploration.ipynb), consentendo l'estrazione delle informazioni necessarie per popolare l'oggetto RDD di Spark."
   ]
  },
  {
   "cell_type": "code",
   "id": "51c8479048aa4b7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T15:06:22.112190Z",
     "start_time": "2025-01-16T15:06:21.359060Z"
    }
   },
   "source": [
    "import java.text.SimpleDateFormat\n",
    "import java.util.Calendar\n",
    "\n",
    "object FlightParser extends Serializable {\n",
    "  \n",
    "  val commaRegex = \",(?=(?:[^\\\"]*\\\"[^\\\"]*\\\")*[^\\\"]*$)\"\n",
    "  val comma = \",\"\n",
    "\n",
    "  /**\n",
    "   * Convert from date (String) to month (Int).\n",
    "   * @param dateString the date\n",
    "   * @return the month\n",
    "   */\n",
    "  def monthFromDate(dateString: String): Int = {\n",
    "    val sdf = new SimpleDateFormat(\"yyyy-MM-dd\")\n",
    "    val date = sdf.parse(dateString.trim)\n",
    "    val cal = Calendar.getInstance()\n",
    "    cal.setTime(date)\n",
    "    cal.get(Calendar.MONTH) + 1\n",
    "  }\n",
    "\n",
    "  case class Flight(\n",
    "     legId: String,\n",
    "     searchDate: Int,\n",
    "     flightDate: Int,\n",
    "     startingAirport: String,\n",
    "     destinationAirport: String,\n",
    "     fareBasisCode: String,\n",
    "     travelDuration: String,\n",
    "     elapsedDays: Int,\n",
    "     isBasicEconomy: Boolean,\n",
    "     isRefundable: Boolean,\n",
    "     isNonStop: Boolean,\n",
    "     baseFare: Double,\n",
    "     totalFare: Double,\n",
    "     seatsRemaining: Int,\n",
    "     totalTravelDistance: Double,\n",
    "     segmentsDepartureTimeEpochSeconds: String,\n",
    "     segmentsDepartureTimeRaw: String,\n",
    "     segmentsArrivalTimeEpochSeconds: String,\n",
    "     segmentsArrivalTimeRaw: String,\n",
    "     segmentsArrivalAirportCode: String,\n",
    "     segmentsDepartureAirportCode: String,\n",
    "     segmentsAirlineName: String,\n",
    "     segmentsAirlineCode: String,\n",
    "     segmentsEquipmentDescription: String,\n",
    "     segmentsDurationInSeconds: String,\n",
    "     segmentsDistance: String,\n",
    "     segmentsCabinCode: String\n",
    "  ) extends Serializable\n",
    "\n",
    "  /**\n",
    "   * Function to parse flights records.\n",
    "   * @param line that has to be parsed\n",
    "   * @return Flight object, None in case of input errors\n",
    "   */\n",
    "  def parseFlightLine(line: String): Option[Flight] = {\n",
    "    try {\n",
    "      val columns = line.split(comma)\n",
    "      Some(\n",
    "        Flight(\n",
    "          legId = columns(0).trim,\n",
    "          searchDate = monthFromDate(columns(1)),\n",
    "          flightDate = monthFromDate(columns(2)),\n",
    "          startingAirport = columns(3).trim,\n",
    "          destinationAirport = columns(4).trim,\n",
    "          fareBasisCode = columns(5).trim,\n",
    "          travelDuration = columns(6).trim,\n",
    "          elapsedDays = columns(7).trim.toInt,\n",
    "          isBasicEconomy = columns(8).trim.toBoolean,\n",
    "          isRefundable = columns(9).trim.toBoolean,\n",
    "          isNonStop = columns(10).trim.toBoolean,\n",
    "          baseFare = columns(11).trim.toDouble,\n",
    "          totalFare = columns(12).trim.toDouble,\n",
    "          seatsRemaining = columns(13).trim.toInt,\n",
    "          totalTravelDistance = columns(14).trim.toDouble,\n",
    "          segmentsDepartureTimeEpochSeconds = columns(15).trim,\n",
    "          segmentsDepartureTimeRaw = columns(16).trim,\n",
    "          segmentsArrivalTimeEpochSeconds = columns(17).trim,\n",
    "          segmentsArrivalTimeRaw = columns(18).trim,\n",
    "          segmentsArrivalAirportCode = columns(19).trim,\n",
    "          segmentsDepartureAirportCode = columns(20).trim,\n",
    "          segmentsAirlineName = columns(21).trim,\n",
    "          segmentsAirlineCode = columns(22).trim,\n",
    "          segmentsEquipmentDescription = columns(23).trim,\n",
    "          segmentsDurationInSeconds = columns(24).trim,\n",
    "          segmentsDistance = columns(25).trim,\n",
    "          segmentsCabinCode = columns(26).trim\n",
    "        )\n",
    "      )\n",
    "    } catch {\n",
    "      case e: Exception =>\n",
    "        // println(s\"Errore durante il parsing della riga '$line': ${e.getMessage}\")\n",
    "        None\n",
    "    }\n",
    "  }\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.text.SimpleDateFormat\r\n",
       "import java.util.Calendar\r\n",
       "defined object FlightParser\r\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "d195400e1cea1b18",
   "metadata": {},
   "source": [
    "### Caricamento dei dati\n",
    "\n",
    "Si effettua il caricamento di uno dei file *itineraries-sample\\<NN\\>.csv* scaricabile dalla cartella su [OneDrive](https://liveunibo-my.sharepoint.com/:f:/g/personal/giulia_nardicchia_studio_unibo_it/Ei2686kRO3JFrY-4LnImGpwBtge9FRErDnIgvT2h2QB-Pg?e=VrufWl)."
   ]
  },
  {
   "cell_type": "code",
   "id": "8681b935c9353693",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T15:06:23.967402Z",
     "start_time": "2025-01-16T15:06:23.838082Z"
    }
   },
   "source": [
    "val datasetsPath = \"../../../../datasets/big/\"\n",
    "val fileName = \"itineraries-sample02.csv\"\n",
    "\n",
    "val rawData = sc.textFile(datasetsPath + fileName)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasetsPath: String = ../../../../datasets/big/\r\n",
       "fileName: String = itineraries-sample02.csv\r\n",
       "rawData: org.apache.spark.rdd.RDD[String] = ../../../../datasets/big/itineraries-sample02.csv MapPartitionsRDD[41] at textFile at <console>:42\r\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Trasformazione di un RDD composto da dati grezzi (*rawData*) in un RDD di oggetti `Flight`. La funzione `FlightParser.parseFlightLine` analizza ogni riga. `flatMap` appiattisce i risultati, scartando automaticamente le righe non valide.",
   "id": "55ad3c19d0de51fa"
  },
  {
   "cell_type": "code",
   "id": "ba8c35cd3020a7a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T15:06:27.089452Z",
     "start_time": "2025-01-16T15:06:26.687498Z"
    }
   },
   "source": "val rddFlights = rawData.flatMap(FlightParser.parseFlightLine)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddFlights: org.apache.spark.rdd.RDD[FlightParser.Flight] = MapPartitionsRDD[42] at flatMap at <console>:38\r\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Numero di righe valide calcolate con `count()`.",
   "id": "adcd44f9230e1434"
  },
  {
   "cell_type": "code",
   "id": "411a20cf90856722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T15:01:41.483127Z",
     "start_time": "2025-01-16T15:01:37.844152Z"
    }
   },
   "source": "rddFlights.count()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Long = 1520662\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Job non ottimizzato",
   "id": "12ae884fea7de77f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Prima aggregazione\n",
    "\n",
    "Aggregare per ogni combinazione di aeroporto di partenza e destinazione (*startingAeroport* e *destinationAeroport*) per ottenere la distanza media di viaggio (*totalTravelDistance*)."
   ],
   "id": "1500dc766c51eab"
  },
  {
   "cell_type": "code",
   "id": "fb3928b8618a571d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T15:16:05.379769Z",
     "start_time": "2025-01-16T15:16:05.163495Z"
    }
   },
   "source": [
    "val flightsDistances = rddFlights\n",
    "  .map(x => ((x.startingAirport, x.destinationAirport), x.totalTravelDistance))\n",
    "  .aggregateByKey((0.0, 0))(\n",
    "    (acc, distance) => (acc._1 + distance, acc._2 + 1), // Somma distanza totale e incrementa conteggio\n",
    "    (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2) // Combina risultati parziali\n",
    "  )\n",
    "  .mapValues { case (sum, count) => (sum / count) } // Calcolo media"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightsDistances: org.apache.spark.rdd.RDD[((String, String), Double)] = MapPartitionsRDD[47] at mapValues at <console>:43\r\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "id": "73c86e45f2fc95d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T15:16:14.651812Z",
     "start_time": "2025-01-16T15:16:10.555824Z"
    }
   },
   "source": [
    "flightsDistances.collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res26: Array[((String, String), Double)] = Array(((BOS,LGA),406.6977958842578), ((IAD,ORD),841.525204359673), ((EWR,PHL),1039.5994575045208), ((DTW,LGA),694.1526669795088), ((OAK,DFW),2123.889001864091), ((ATL,DEN),1513.575124745888), ((IAD,CLT),587.9657102869139), ((DEN,LGA),1804.57909562639), ((DTW,EWR),736.2682451253482), ((LGA,DFW),1455.155069582505), ((OAK,JFK),3126.575707702436), ((DEN,DTW),1578.6580700623254), ((JFK,IAD),703.4175354183374), ((ORD,MIA),1521.1334047682828), ((IAD,DFW),1363.3681891954557), ((DEN,PHL),1852.40172900494), ((OAK,DEN),1419.471807628524), ((BOS,JFK),261.94046744083494), ((SFO,JFK),2652.746982695943), ((DTW,MIA),1462.8436163714111), ((PHL,OAK),2949.3589503280223), ((CLT,LGA),665.3443708609271), ((DTW,JFK),842.9147381242387), ((ATL,IAD),647.0074156470153), ...\r\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A partire dalla distanza media generare una nuova colonna che indichi la fascia di distanza del volo (breve distanza, media distanza, lunga distanza).\n",
    "\n",
    "Poiché usare valori numerici *hard coded* è una *bad practice*, si è deciso di utilizzare il minimo, il massimo e il numero di classi per calcolare dinamicamente l'intervallo delle fasce di distanza."
   ],
   "id": "82f64718cfaaeafc"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51ab6cf21a842b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minDistance: Double = 993.9075896762905\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val minDistance = flightsDistances.min()._2._1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59a99f04f05d1d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "maxDistance: Double = 2738.66144486692\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val maxDistance = flightsDistances.max()._2._1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d113be73d8f55809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numClassi: Int = 3\r\n",
       "range: Double = 581.5846183968765\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numClassi = 3\n",
    "\n",
    "val range = (maxDistance - minDistance) / numClassi"
   ]
  },
  {
   "cell_type": "code",
   "id": "589f908d9810a6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T15:20:47.351439Z",
     "start_time": "2025-01-16T15:20:46.536154Z"
    }
   },
   "source": [
    "val classifiedDistances = flightsDistances.map {\n",
    "  case ((startingAirport, destinationAirport), avgDistance) =>\n",
    "    val classification = if (avgDistance < minDistance + range) \"Breve\"\n",
    "    else if (avgDistance <= minDistance + (numClassi - 1) * range ) \"Media\"\n",
    "    else \"Lunga\"\n",
    "    ((startingAirport, destinationAirport), classification)\n",
    "}"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifiedDistances: org.apache.spark.rdd.RDD[((String, String), String)] = MapPartitionsRDD[48] at map at <console>:40\r\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "id": "4553525b-378c-418d-bd9e-713373f4d97b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T15:20:50.489733Z",
     "start_time": "2025-01-16T15:20:49.620365Z"
    }
   },
   "source": [
    "classifiedDistances.collect()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res27: Array[((String, String), String)] = Array(((BOS,LGA),Breve), ((IAD,ORD),Breve), ((EWR,PHL),Breve), ((DTW,LGA),Breve), ((OAK,DFW),Media), ((ATL,DEN),Breve), ((IAD,CLT),Breve), ((DEN,LGA),Media), ((DTW,EWR),Breve), ((LGA,DFW),Breve), ((OAK,JFK),Lunga), ((DEN,DTW),Media), ((JFK,IAD),Breve), ((ORD,MIA),Breve), ((IAD,DFW),Breve), ((DEN,PHL),Media), ((OAK,DEN),Breve), ((BOS,JFK),Breve), ((SFO,JFK),Lunga), ((DTW,MIA),Breve), ((PHL,OAK),Lunga), ((CLT,LGA),Breve), ((DTW,JFK),Breve), ((ATL,IAD),Breve), ((ATL,MIA),Breve), ((DTW,IAD),Breve), ((OAK,LGA),Lunga), ((SFO,EWR),Lunga), ((IAD,SFO),Lunga), ((CLT,SFO),Lunga), ((BOS,ATL),Breve), ((LAX,DEN),Breve), ((DEN,JFK),Media), ((BOS,LAX),Lunga), ((SFO,IAD),Lunga), ((DTW,DEN),Breve), ((ORD,LGA),Breve), ((ATL,OAK),Lunga), ((MIA,CLT),Breve), ((EWR,L...\r\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Join + Seconda aggregazione\n",
    "\n",
    "Unire il dataset originale con il risultato ottenuto e aggregare per fascia di distanza e mese (*flightDate*, da cui si ricava il mese) per ottenere per ciascuna combinazione il prezzo medio."
   ],
   "id": "c24023d49bd4355e"
  },
  {
   "cell_type": "code",
   "id": "bc88d7785805c488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T15:34:19.938437Z",
     "start_time": "2025-01-16T15:34:19.609023Z"
    }
   },
   "source": [
    "val resultJobNotOptimized = rddFlights\n",
    "  .map(x => ((x.startingAirport, x.destinationAirport), (x.flightDate, x.totalFare)))\n",
    "  .join(classifiedDistances)\n",
    "  .map {\n",
    "    case ((startingAirport, destinationAirport), ((month, totalFare), classification)) => ((month, classification), (totalFare, 1))\n",
    "  }\n",
    "  .aggregateByKey((0.0, 0))(\n",
    "    (acc, value) => (acc._1 + value._1, acc._2 + value._2),\n",
    "    (acc1, acc2) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n",
    "  )\n",
    "  .map {\n",
    "    case ((month, classification), (sum, count)) => (month, classification, sum / count)\n",
    "  }"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result: org.apache.spark.rdd.RDD[(Int, String, Double)] = MapPartitionsRDD[73] at map at <console>:48\r\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "id": "b44ce1760987d212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T15:34:26.921592Z",
     "start_time": "2025-01-16T15:34:21.249840Z"
    }
   },
   "source": "resultJobNotOptimized.collect()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res33: Array[(Int, String, Double)] = Array((5,Breve,283.8788881446023), (10,Media,325.7054848013336), (5,Media,413.6171160589015), (8,Lunga,461.10027818585945), (6,Breve,304.4254653392809), (11,Media,272.2007247531067), (11,Breve,222.67424202278337), (7,Lunga,553.1568957980675), (5,Lunga,531.5929277671767), (9,Media,326.2977310691745), (9,Breve,255.8687988591393), (6,Media,458.49081160607454), (11,Lunga,383.3642073607308), (7,Breve,299.0520991699244), (10,Lunga,410.4446781736263), (7,Media,432.4863157403677), (8,Media,359.8989032783995), (4,Breve,305.31267008117635), (6,Lunga,597.482866518744), (8,Breve,268.6789576042566), (10,Breve,259.3999845308579), (4,Media,356.7572332890661), (4,Lunga,480.69310774341034), (9,Lunga,404.9251304499109))\r\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Job ottimizzato",
   "id": "86485626af9c583f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Salvataggio dei risultati su file",
   "id": "78bfd7ca0e66fbb2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a687d9ab92eb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SaveMode\n",
    "\n",
    "val jobNotOptimized = \"../../../../output/jobNotOptimized\"\n",
    "val jobOptimized = \"../../../../output/jobOptimized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf1c58b0a76798",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultJobNotOptimized\n",
    "  .coalesce(1)\n",
    "  .toDF().write.format(\"csv\").mode(SaveMode.Overwrite).save(jobNotOptimized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
